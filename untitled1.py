# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15SBWHDVF9ngtvOsoXLZh1Wo7c7yPP_i0
"""

import os
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import Xception
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.metrics import classification_report

# Define paths to datasets
data_dir = r'C:\Users\kevin\PycharmProjects\termpaper\.venv\Lib\archive\1000_videos'  # Update with your dataset root directory
train_dir = os.path.join(data_dir, 'train')
val_dir = os.path.join(data_dir, 'validation')
test_dir = os.path.join(data_dir, 'test')

# Define image size and batch size
IMG_SIZE = 299
BATCH_SIZE = 32

# Data augmentation and normalization for training and validation
data_gen_args = {
    'rescale': 1./127.5 - 1,  # Normalize to [-1, 1]
    'horizontal_flip': True,
    'rotation_range': 10,
    'zoom_range': 0.1
}

train_datagen = ImageDataGenerator(**data_gen_args)
validation_datagen = ImageDataGenerator(rescale=1./127.5 - 1)
test_datagen = ImageDataGenerator(rescale=1./127.5 - 1)

# Data generators
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='binary'
)

validation_generator = validation_datagen.flow_from_directory(
    val_dir,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='binary'
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='binary'
)

# Build the Xception-based model
def build_xception_model(input_shape=(IMG_SIZE, IMG_SIZE, 3)):
    base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)
    base_model.trainable = False  # Freeze the pre-trained base model

    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(1, activation='sigmoid')  # Binary classification (real/fake)
    ])

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model, base_model  # Return both model and base_model

# Initialize the model
model, base_model = build_xception_model()  # Get both objects
model.summary()

# Callbacks for training
checkpoint = ModelCheckpoint('xception_model_best.keras', save_best_only=True, monitor='val_loss', mode='min')
early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='min')

# Train the model
history = model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=10,  # Use more epochs for better results
    callbacks=[early_stopping]
)

# Fine-tuning (unfreeze some layers of Xception)
base_model.trainable = True
for layer in base_model.layers[:100]:  # Freeze the first 100 layers
    layer.trainable = False

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
              loss='binary_crossentropy',
              metrics=['accuracy'])

history_fine = model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=10,  # Use more epochs for better results
    callbacks=[checkpoint, early_stopping]
)

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(test_generator)
print(f"Test Accuracy: {test_acc}")

# Generate classification report
y_true = test_generator.classes
y_pred = (model.predict(test_generator) > 0.5).astype('int32')

print(classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys()))

# Save the final model
model.save('xception_model_best.keras')
model.save('xception_model_best.h5')